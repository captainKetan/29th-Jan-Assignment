{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bad8b5a-b3ab-45d3-ad5a-2669b80121b9",
   "metadata": {},
   "source": [
    " # Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797bd4e-7895-4d1b-99c7-b8df09b1ba6e",
   "metadata": {},
   "source": [
    "Lasso Regression, or L1 regularization, is a linear regression technique that combines the principles of least squares regression with a regularization term. The regularization term is based on the absolute values of the coefficients, and it is added to the ordinary least squares (OLS) cost function. The objective of Lasso Regression is to minimize the sum of squared residuals while keeping the sum of the absolute values of the coefficients small.\n",
    "\n",
    "Key features of Lasso Regression and its differences from other regression techniques:\n",
    "\n",
    "1. **Regularization Term (L1 Penalty):**\n",
    "   - Lasso Regression adds a regularization term to the cost function, which is the sum of the absolute values of the coefficients multiplied by a regularization parameter (lambda or α). This term is added to the OLS cost function and is responsible for shrinking the coefficients towards zero.\n",
    "\n",
    "2. **Variable Selection:**\n",
    "   - One distinctive feature of Lasso Regression is that it can lead to variable selection. Due to the nature of the L1 penalty, some coefficients can be exactly zero, effectively excluding certain predictors from the model. This property makes Lasso Regression useful for feature selection and sparse models.\n",
    "\n",
    "3. **Shrinkage of Coefficients:**\n",
    "   - Lasso Regression shrinks the coefficients towards zero, similar to Ridge Regression. However, the L1 penalty has a tendency to result in more coefficients being exactly zero, leading to sparsity. This contrasts with Ridge Regression, which tends to shrink coefficients towards but not exactly to zero.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Like Ridge Regression, Lasso Regression is effective in handling multicollinearity by shrinking coefficients. However, Lasso's ability to select variables can be advantageous in situations where feature selection is desired due to correlated predictors.\n",
    "\n",
    "5. **Loss Function:**\n",
    "   - The overall loss function in Lasso Regression is the sum of the least squares loss (sum of squared residuals) and the L1 penalty term. The loss function to be minimized is given by:\n",
    "      {Loss} = {OLS Loss} + lambda sum_{i=1}^{n}|beta_i|\n",
    "      where lambda is the regularization parameter.\n",
    "\n",
    "6. **Use Case:**\n",
    "   - Lasso Regression is particularly useful when dealing with high-dimensional datasets with many predictors. It is well-suited for situations where a subset of predictors is expected to have a significant impact on the response variable, and the rest may be irrelevant or redundant.\n",
    "\n",
    "7. **Difference from Ridge Regression:**\n",
    "   - The primary difference between Lasso and Ridge Regression lies in the regularization term. While Ridge uses the squared magnitudes of coefficients (L2 penalty), Lasso uses the absolute values of coefficients (L1 penalty). The L1 penalty tends to encourage sparsity and variable selection, whereas the L2 penalty tends to shrink coefficients more evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94018ff9-987c-43e8-b3b4-68e64100a17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1af2562b-89c6-4f5b-a3ea-a407a3a2b6ee",
   "metadata": {},
   "source": [
    " # Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f358bd1-764c-4276-9dfb-1adc6a100deb",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection lies in its ability to automatically perform variable selection by setting some of the coefficients to exactly zero. This property makes Lasso particularly useful when dealing with high-dimensional datasets with a large number of predictors. Here are the key advantages of Lasso Regression in the context of feature selection:\n",
    "\n",
    "1. **Sparsity and Automatic Variable Selection:**\n",
    "   - Lasso introduces an L1 penalty term to the cost function, which is the sum of the absolute values of the coefficients multiplied by a regularization parameter (lambda or α). This penalty has the effect of shrinking some coefficients towards zero. In cases where the penalty is strong enough, some coefficients become exactly zero, resulting in a sparse model. This sparsity leads to automatic variable selection.\n",
    "\n",
    "2. **Reduction of Overfitting:**\n",
    "   - The sparsity induced by Lasso helps reduce overfitting by excluding irrelevant or redundant predictors from the model. In high-dimensional datasets, where the number of predictors is close to or exceeds the number of observations, traditional regression models may struggle with overfitting. Lasso's ability to set some coefficients to zero mitigates this issue and results in a more parsimonious model.\n",
    "\n",
    "3. **Improved Model Interpretability:**\n",
    "   - The sparsity introduced by Lasso not only reduces the number of predictors in the model but also facilitates model interpretability. A simpler model with fewer variables can be easier to understand and interpret, making it more practical for communication and decision-making.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Lasso Regression is effective in handling multicollinearity, which occurs when predictors are highly correlated. By setting some coefficients to zero, Lasso can choose one variable over another in cases of high correlation. This can be advantageous when dealing with redundant predictors.\n",
    "\n",
    "5. **Feature Subset Selection:**\n",
    "   - Lasso allows for the identification of a subset of features that have the most significant impact on the response variable. This can be valuable in scenarios where the goal is to identify a concise set of predictors that contribute meaningfully to the model.\n",
    "\n",
    "6. **Useful in Machine Learning Pipelines:**\n",
    "   - Lasso Regression is often used as a component in machine learning pipelines, where feature selection is an integral step. It can be combined with other techniques to build more robust and interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f488c-f4e0-407c-a0a1-929f5450eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47dc0467-4099-4681-a7f6-3c0c4102566a",
   "metadata": {},
   "source": [
    " # Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e821d8-3b28-4620-9c37-4e1a59837016",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves considering the magnitude, sign, and sparsity of the coefficients. Lasso Regression, with its L1 penalty, encourages sparsity by setting some coefficients exactly to zero. Here's how to interpret the coefficients in a Lasso Regression model:\n",
    "\n",
    "1. **Magnitude of Coefficients:**\n",
    "   - The magnitude of each coefficient represents the strength of the relationship between the corresponding predictor and the response variable. Larger absolute values indicate a stronger impact on the response variable.\n",
    "\n",
    "2. **Sign of Coefficients:**\n",
    "   - The sign of each coefficient indicates the direction of the relationship. A positive coefficient suggests a positive relationship (as the predictor increases, the response variable tends to increase), while a negative coefficient suggests a negative relationship.\n",
    "\n",
    "3. **Variable Selection:**\n",
    "   - One of the unique features of Lasso Regression is its ability to set some coefficients exactly to zero. Coefficients that are zero imply that the corresponding predictors are not contributing to the model. The non-zero coefficients are the selected variables that have a non-negligible impact on the response.\n",
    "\n",
    "4. **Subset of Relevant Predictors:**\n",
    "   - The non-zero coefficients in a Lasso model represent a subset of relevant predictors. This subset is selected based on the strength of their relationship with the response variable. The sparsity induced by Lasso is especially useful in situations with many predictors, where automatic variable selection is desired.\n",
    "\n",
    "5. **Effect of Regularization Parameter (Lambda):**\n",
    "   - The regularization parameter (lambda or α) in Lasso controls the strength of the penalty applied to the absolute values of the coefficients. As the regularization parameter increases, more coefficients tend to be set to zero. The choice of the regularization parameter influences the sparsity of the model and the trade-off between model fit and simplicity.\n",
    "\n",
    "6. **Handling Multicollinearity:**\n",
    "   - Lasso Regression can handle multicollinearity by effectively selecting one variable over another when they are highly correlated. In the presence of correlated predictors, Lasso tends to favor one of the predictors and set the others to zero.\n",
    "\n",
    "7. **Interpretation Challenges:**\n",
    "   - Due to the sparsity introduced by Lasso, the interpretation of individual coefficients can be challenging, especially for predictors with coefficients set to zero. In practice, the focus may be on the overall contribution of the selected variables rather than detailed interpretations of excluded predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c6d44-a419-4d77-ae97-ad0a27805cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1403059-e7c8-418e-9d04-abb0843965f3",
   "metadata": {},
   "source": [
    " # Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209d62b-3fc7-48f1-a581-158df0d9be04",
   "metadata": {},
   "source": [
    "In Lasso Regression, the tuning parameter that can be adjusted is often denoted as lambda (λ) or alpha (α). This parameter controls the strength of the regularization applied to the model. The tuning parameter influences the balance between fitting the data well (reducing the sum of squared residuals) and penalizing the absolute values of the coefficients.\n",
    "\n",
    "There are two common ways to express the tuning parameter in Lasso Regression:\n",
    "\n",
    "1. **Lambda (λ):**\n",
    "   - Lambda is a positive constant that multiplies the L1 penalty term in the Lasso cost function. The higher the value of λ, the stronger the penalty, leading to more shrinkage of coefficients and potentially more coefficients being set to zero.\n",
    "\n",
    "2. **Alpha (α):**\n",
    "   - Alpha is a parameter that represents the mixing of L1 and L2 penalties in the cost function. The relationship between lambda and alpha is given by:\n",
    "      \\[ \\lambda = \\alpha \\times \\text{sum of squared residuals} \\]\n",
    "   - Alpha takes values between 0 and 1. When alpha is 0, Lasso Regression is equivalent to Ridge Regression (L2 penalty), and as alpha approaches 1, the penalty becomes purely L1 (Lasso).\n",
    "\n",
    "The tuning parameter in Lasso Regression affects the model's performance in the following ways:\n",
    "\n",
    "1. **Magnitude of Coefficients:**\n",
    "   - As the tuning parameter (λ or α) increases, the penalty on the absolute values of the coefficients becomes stronger. This leads to more shrinkage of coefficients, and some coefficients may be set exactly to zero. The magnitude of the coefficients is reduced, contributing to a simpler and more interpretable model.\n",
    "\n",
    "2. **Sparsity and Variable Selection:**\n",
    "   - A higher tuning parameter encourages sparsity in the model by setting more coefficients to exactly zero. This has the effect of performing automatic variable selection, as predictors with zero coefficients are essentially excluded from the model. Lower values of the tuning parameter may result in fewer coefficients being exactly zero.\n",
    "\n",
    "3. **Overfitting and Model Complexity:**\n",
    "   - A lower tuning parameter allows the model to fit the training data more closely, potentially leading to overfitting, especially in the presence of many predictors. On the other hand, a higher tuning parameter introduces more regularization, preventing overfitting and promoting a simpler model.\n",
    "\n",
    "4. **Trade-off Between Fit and Regularization:**\n",
    "   - The choice of the tuning parameter involves a trade-off between fitting the data well and preventing overfitting. Cross-validation techniques, such as k-fold cross-validation, can be employed to find the optimal value of the tuning parameter that maximizes model performance on unseen data.\n",
    "\n",
    "5. **Sensitivity to Outliers:**\n",
    "   - Lasso Regression is sensitive to outliers, and the choice of the tuning parameter can influence the model's robustness to extreme values. Higher values of the tuning parameter may provide more robustness against outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870d788-69a4-44cf-ad52-6782b15138d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4124f9fa-d654-4a38-8699-c63617651f66",
   "metadata": {},
   "source": [
    " # Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ae42a-0d17-478e-a901-5e7f7895f0f9",
   "metadata": {},
   "source": [
    "Lasso Regression, by itself, is a linear regression technique designed for problems where the relationship between the predictors and the response variable is linear. It involves minimizing the sum of squared residuals with the addition of an L1 regularization term. This makes Lasso Regression particularly effective in situations where there are many predictors and some of them may not be relevant.\n",
    "\n",
    "For non-linear regression problems, where the relationship between predictors and the response variable is non-linear, Lasso Regression may not be directly applicable. However, it is possible to extend Lasso Regression for non-linear regression problems by incorporating non-linear transformations of the predictors.\n",
    "\n",
    "Here's how you can use Lasso Regression for non-linear regression problems:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Introduce non-linear transformations of the predictors. For example, you can create new features by taking the square, square root, logarithm, or other non-linear transformations of existing predictors. These transformed features can capture non-linear relationships.\n",
    "\n",
    "2. **Polynomial Regression:**\n",
    "   - A common approach to handling non-linear relationships is to use Polynomial Regression. In this approach, you introduce polynomial terms (e.g., \\(X^2\\), \\(X^3\\)) as additional predictors. Lasso Regression can then be applied to the extended set of predictors, allowing it to select relevant polynomial terms and regularize the model.\n",
    "\n",
    "3. **Interaction Terms:**\n",
    "   - Include interaction terms between predictors. Interaction terms capture the combined effect of two or more predictors and can help model non-linear relationships. Lasso Regression can be applied to a model with interaction terms.\n",
    "\n",
    "4. **Use Kernelized Methods:**\n",
    "   - Kernelized methods, such as the Support Vector Regression with a kernel function or the kernel trick, can be used to implicitly capture non-linear relationships. These methods transform the input space into a higher-dimensional space, making it possible to model non-linear relationships.\n",
    "\n",
    "5. **Combine Lasso with Non-Linear Models:**\n",
    "   - Instead of directly applying Lasso Regression, you can use an ensemble or stacking approach. Train separate non-linear models, such as decision trees, random forests, or gradient boosting, and combine their predictions. Apply Lasso Regression as a meta-model to regularize the ensemble.\n",
    "\n",
    "6. **Regularization for Non-Linear Models:**\n",
    "   - If you choose to use non-linear regression models, such as neural networks, you can still apply regularization techniques within those models. Many non-linear models have regularization parameters that control the complexity of the model and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7e9cc-36fa-420f-91eb-7de43db32cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9e8936-8b46-458c-9206-e4e76404fcc9",
   "metadata": {},
   "source": [
    " # Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2823feb-779b-4463-bd35-e9e19fe9b13a",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to address the issues of multicollinearity and overfitting. Despite sharing similarities, they differ in their regularization approaches, and each has its own strengths and characteristics. Here are the key differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - **Ridge Regression:** It adds a regularization term to the cost function, which is the sum of squared magnitudes of the coefficients multiplied by a regularization parameter (lambda or α). The regularization term takes the form of \\(\\lambda \\sum_{i=1}^{n}\\beta_i^2\\), where \\(\\beta_i\\) represents the coefficients.\n",
    "   - **Lasso Regression:** It adds a regularization term to the cost function, which is the sum of the absolute values of the coefficients multiplied by a regularization parameter. The regularization term takes the form of \\(\\lambda \\sum_{i=1}^{n}|\\beta_i|\\).\n",
    "\n",
    "2. **Penalty Type:**\n",
    "   - **Ridge Regression:** It uses an L2 penalty, which penalizes the squared magnitudes of the coefficients. The L2 penalty tends to shrink all coefficients towards zero proportionally.\n",
    "   - **Lasso Regression:** It uses an L1 penalty, which penalizes the absolute values of the coefficients. The L1 penalty has a tendency to shrink some coefficients exactly to zero, effectively performing variable selection.\n",
    "\n",
    "3. **Variable Selection:**\n",
    "   - **Ridge Regression:** It tends to shrink coefficients towards zero, but it rarely sets coefficients exactly to zero. Ridge Regression is less likely to result in sparse models and variable selection.\n",
    "   - **Lasso Regression:** It can lead to sparse models by setting some coefficients exactly to zero. Lasso Regression is particularly useful for feature selection, as it can identify a subset of relevant predictors.\n",
    "\n",
    "4. **Solution for Multicollinearity:**\n",
    "   - **Ridge Regression:** It is effective in handling multicollinearity by distributing the impact of correlated predictors across all coefficients. Ridge tends to shrink correlated predictors towards each other.\n",
    "   - **Lasso Regression:** It is also effective in handling multicollinearity, but it has the ability to select one predictor over another when they are highly correlated. Lasso tends to favor one of the correlated predictors and set the others to zero.\n",
    "\n",
    "5. **Loss Function:**\n",
    "   - **Ridge Regression:** The overall loss function in Ridge Regression is the sum of the least squares loss (sum of squared residuals) and the L2 penalty term.\n",
    "   - **Lasso Regression:** The overall loss function in Lasso Regression is the sum of the least squares loss and the L1 penalty term.\n",
    "\n",
    "6. **Geometric Interpretation:**\n",
    "   - **Ridge Regression:** The contours of the ridge penalty form circles in the coefficient space. The solution is found at the intersection of the contours and the ellipses of the least squares loss.\n",
    "   - **Lasso Regression:** The contours of the lasso penalty form diamond shapes in the coefficient space. The solution is often found at the corners of these diamonds.\n",
    "\n",
    "7. **Computational Complexity:**\n",
    "   - **Ridge Regression:** The solution to Ridge Regression can be obtained through closed-form equations, making it computationally efficient.\n",
    "   - **Lasso Regression:** The solution to Lasso Regression involves solving a more complex optimization problem, and it may require iterative algorithms such as coordinate descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f7f83-9de8-47f4-8d97-cdd964eb0e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fa44c5-c57a-4e0b-b0db-2618e08560f4",
   "metadata": {},
   "source": [
    " # Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41063483-b9cb-4f87-a921-a165d52de579",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, and it does so through its inherent property of variable selection. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, making it challenging to separate their individual effects on the response variable. Lasso Regression, with its L1 regularization penalty, has the ability to address multicollinearity in the following ways:\n",
    "\n",
    "1. **Variable Selection:**\n",
    "   - Lasso Regression has a sparsity-inducing property, meaning that it can set some of the coefficients exactly to zero. When faced with highly correlated predictors, Lasso tends to select one predictor over another by setting the coefficient of one of them to zero. This process effectively performs automatic variable selection.\n",
    "\n",
    "2. **Shrinking Coefficients:**\n",
    "   - The L1 penalty in Lasso Regression penalizes the absolute values of the coefficients. As the strength of the penalty increases (controlled by the regularization parameter lambda or α), Lasso tends to shrink some coefficients towards zero. In the presence of multicollinearity, this shrinkage helps distribute the impact of correlated predictors, preventing them from having disproportionately large coefficients.\n",
    "\n",
    "3. **Subset of Relevant Predictors:**\n",
    "   - Lasso Regression identifies a subset of relevant predictors that have non-zero coefficients. When multicollinearity is present, Lasso may select one of the correlated predictors while setting the others to zero. The subset of selected predictors represents a sparse set of variables that collectively contribute to the model.\n",
    "\n",
    "4. **Promoting Simplicity:**\n",
    "   - The sparsity induced by Lasso not only helps with variable selection but also contributes to model simplicity. By excluding some predictors, Lasso produces a simpler model that is easier to interpret and less prone to overfitting, especially when dealing with a large number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abaf68-60a5-49e3-b397-4dfa1efff50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb24230-ffbb-4f27-b62c-0e2d461fc9d8",
   "metadata": {},
   "source": [
    " # Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead524f-d564-4e53-8277-a2f040758ddc",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (lambda or α) in Lasso Regression involves finding a balance between model fit and regularization. The goal is to select a value that minimizes prediction error on new, unseen data while preventing overfitting. Here are common methods for choosing the optimal value of the regularization parameter in Lasso Regression:\n",
    "\n",
    "1. **Cross-Validation:**\n",
    "   - **K-Fold Cross-Validation:** Split the dataset into k folds, train the Lasso Regression model on k-1 folds, and validate it on the remaining fold. Repeat this process k times, each time using a different fold as the validation set. Calculate the average prediction error (e.g., mean squared error) for each value of the regularization parameter. Choose the value of λ that minimizes the average prediction error.\n",
    "\n",
    "   - **Leave-One-Out Cross-Validation (LOOCV):** A special case of k-fold cross-validation where k is set to the number of samples. For each iteration, one sample is used as the validation set, and the model is trained on the remaining samples. This process is repeated for each sample, and the average prediction error is calculated.\n",
    "\n",
    "2. **Grid Search:**\n",
    "   - Manually define a range of values for the regularization parameter and perform Lasso Regression with each value. Evaluate the model's performance using a performance metric (e.g., mean squared error) on a validation set or through cross-validation. Choose the regularization parameter value that results in the best model performance.\n",
    "\n",
    "3. **Regularization Path:**\n",
    "   - Examine the regularization path, which shows how the coefficients change as the regularization parameter varies. Plot the coefficients against the log-scale of the regularization parameter. Identify the point at which certain coefficients become exactly zero, indicating variable selection. This can provide insights into the sparsity-inducing nature of Lasso.\n",
    "\n",
    "4. **Information Criteria:**\n",
    "   - Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used to balance model fit and complexity. These criteria penalize the goodness of fit based on the number of parameters. Choose the regularization parameter that minimizes the information criterion.\n",
    "\n",
    "5. **Coordinate Descent Path:**\n",
    "   - For Lasso Regression, coordinate descent is often used as an optimization algorithm. Examine the path of the regularization parameter during coordinate descent. This can provide insights into the convergence behavior and help identify suitable values for λ.\n",
    "\n",
    "6. **Use of Validation Set:**\n",
    "   - Split the dataset into training and validation sets. Train the Lasso Regression model with different values of λ on the training set and evaluate its performance on the validation set. Choose the regularization parameter that performs well on the validation set.\n",
    "\n",
    "7. **Nested Cross-Validation:**\n",
    "   - Perform nested cross-validation, where an outer loop is used for model evaluation, and an inner loop is used for model selection. The inner loop involves cross-validation to choose the optimal value of λ, and the outer loop evaluates the model's performance on different test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78581105-9df5-449a-aadc-c40374411ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
