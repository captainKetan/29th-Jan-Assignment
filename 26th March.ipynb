{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30d90a9-c7ee-4823-939e-02fd1dc67b49",
   "metadata": {},
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d4e8b-bd7a-4a72-b4f1-baacb6cc7b04",
   "metadata": {},
   "source": [
    "**Simple Linear Regression:**\n",
    "\n",
    "1. **Definition:** Simple linear regression is a statistical method used to model the relationship between a single independent variable and a dependent variable by fitting a linear equation to the data.\n",
    "\n",
    "2. **Equation:** The equation for simple linear regression can be expressed as:  \n",
    "   Y = (theta_0) + (theta_1)X  \n",
    "   - Y is the dependent variable.\n",
    "   - X is the independent variable.\n",
    "   - theta_0 is the intercept (the value of Y when X is 0).\n",
    "   - theta_1 is the slope (the change in Y for a unit change in X).\n",
    "\n",
    "3. **Use Case:** Simple linear regression is used when you want to model the relationship between two variables and make predictions based on that relationship. For example, you might use it to predict a student's test score Y based on the number of hours they studied X.\n",
    "\n",
    "**Multiple Linear Regression:**\n",
    "\n",
    "1. **Definition:** Multiple linear regression is an extension of simple linear regression that models the relationship between multiple independent variables and a dependent variable by fitting a linear equation to the data.\n",
    "\n",
    "2. **Equation:** The equation for multiple linear regression can be expressed as:  \n",
    "   Y = theta_0 + (theta_1)X_1 + (theta_2)X_2 + ...... + (theta_n)X_n\n",
    "   - Y is the dependent variable.\n",
    "   - X_1, X_2, upto , X_n are the independent variables (predictors).\n",
    "   - theta_0 is the intercept.\n",
    "   - theta_1, theta_2, upto , theta_n are the slopes corresponding to each predictor.\n",
    "\n",
    "3. **Use Case:** Multiple linear regression is used when there are multiple predictors that may influence the dependent variable. For example, you might use it to predict a house's sale price Y based on features like the number of bedrooms X_1, square footage X_2, and neighborhood X_3.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "1. **Number of Variables:**\n",
    "   - Simple linear regression has one independent variable and one dependent variable.\n",
    "   - Multiple linear regression has multiple independent variables and one dependent variable.\n",
    "\n",
    "2. **Use Case:**\n",
    "   - Simple linear regression is suitable for modeling and making predictions when there's a straightforward relationship between two variables.\n",
    "   - Multiple linear regression is used when there are multiple factors influencing the outcome, and you want to account for all of them.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Simple Linear Regression:**\n",
    "- **Problem:** Predict a car's fuel efficiency (miles per gallon, Y) based on its weight (in pounds, X).\n",
    "- **Equation:** Y = (theta_0) + (theta_1)X\n",
    "- **Use:** Determine how a car's weight influences its fuel efficiency.\n",
    "\n",
    "**Multiple Linear Regression:**\n",
    "- **Problem:** Predict a house's sale price Y based on its number of bedrooms X_1, square footage X_2, and neighborhood X_3.\n",
    "- **Equation:** Y = (theta_0) + (theta_1)X_1 + (theta_2)X_2 + (theta_3)X_3\n",
    "- **Use:** Consider multiple factors (bedrooms, square footage, neighborhood) to estimate a house's sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b02949-f796-440c-81aa-bf3f7dc35abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc29bcc4-5297-46ac-b6e4-08c905efb33e",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde0cb8-fd22-45ed-8b3e-bc3194542914",
   "metadata": {},
   "source": [
    "Linear regression makes several important assumptions about the relationship between the independent and dependent variables. It's crucial to check whether these assumptions hold when applying linear regression to a dataset. Here are the key assumptions of linear regression:\n",
    "\n",
    "1. **Linearity:** The relationship between the independent variables and the dependent variable should be linear. This means that changes in the predictors are associated with constant changes in the outcome. You can check this assumption by creating scatterplots of the predictors against the dependent variable and looking for a roughly linear pattern.\n",
    "\n",
    "2. **Independence:** The residuals (the differences between the actual and predicted values) should be independent of each other. In other words, the errors in prediction for one data point should not depend on the errors for other data points. You can check this assumption by examining the residuals for autocorrelation (e.g., using a residual plot or autocorrelation function).\n",
    "\n",
    "3. **Homoscedasticity:** The variance of the residuals should be constant across all levels of the predictors. This means that the spread of the residuals should be roughly the same throughout the range of the predictors. You can check this assumption by creating a residual plot or using statistical tests like the Breusch-Pagan test or White's test.\n",
    "\n",
    "4. **No or Little Multicollinearity:** If you're using multiple independent variables, they should not be highly correlated with each other (multicollinearity). High multicollinearity can make it challenging to distinguish the individual effects of predictors. You can check for multicollinearity using correlation matrices.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can perform the following diagnostic tests and visualizations:\n",
    "\n",
    "- **Residual Plots:** Create scatterplots of the residuals against the predicted values or against each predictor. Look for patterns or trends that violate the assumptions.\n",
    "\n",
    "- **Histograms and Q-Q Plots:** Examine histograms of the residuals and Q-Q plots to assess normality. If the residuals deviate significantly from a normal distribution, consider transformations or robust regression techniques.\n",
    "\n",
    "- **Correlation Matrices:** Check for multicollinearity by examining correlation matrices for the predictors. High correlations may indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51117bf-4169-455f-9317-6b0f56e46dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d131db50-80a0-4444-9b83-40c94fcd8d3f",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82dce8-6a21-4cc8-a4ac-db71f4cad9c0",
   "metadata": {},
   "source": [
    "In a linear regression model with a single independent variable (simple linear regression), the model equation is:\n",
    "\n",
    "Y = theta_0 + (theta_1)X\n",
    "\n",
    "Here's how to interpret the slope (theta_1) and intercept (theta_0):\n",
    "\n",
    "1. **Slope (theta_1):**\n",
    "   - The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
    "   - It quantifies the strength and direction of the linear relationship between the independent and dependent variables.\n",
    "   - If theta_1 is positive, it indicates that as X increases, Y is expected to increase.\n",
    "   - If theta_1 is negative, it indicates that as X increases, Y is expected to decrease.\n",
    "   - The magnitude of theta_1 indicates the degree of the effect: larger values suggest a stronger effect, while smaller values suggest a weaker effect.\n",
    "\n",
    "2. **Intercept (theta_0):**\n",
    "   - The intercept is the value of the dependent variable (Y) when the independent variable (X) is zero.\n",
    "   - In many real-world scenarios, an interpretation of the intercept might not have a meaningful interpretation. For example, if X represents the number of years of education, an intercept of (Y = -10) might not make sense because there is no such thing as negative values for education.\n",
    "   - It's often important to focus on the slope (theta_1) for meaningful interpretation.\n",
    "\n",
    "Let's illustrate this with a real-world example:\n",
    "\n",
    "**Scenario:** Suppose you are analyzing the relationship between the number of years of work experience (X) and annual salary (Y) for a group of employees. You fit a simple linear regression model to the data.\n",
    "\n",
    "- **Slope (theta_1):** Let's say you find that theta_1 is 5, meaning for each additional year of work experience, an employee's annual salary is expected to increase by $5,000. This indicates a positive linear relationship between experience and salary.\n",
    "\n",
    "- **Intercept (theta_0):** If the intercept (theta_0) is 30,000 USD, it means that an employee with zero years of work experience (e.g., a recent graduate) is expected to have an annual salary of 30,000 USD.\n",
    "\n",
    "So, you can interpret the model as follows: \"For this group of employees, the estimated starting salary for someone with no work experience is 30,000 USD, and for each additional year of experience, the salary is expected to increase by $5,000.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f946c28-5586-479f-98b6-9ffc0c449936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "152e9ac2-3cc8-4cfe-8bd1-61c0ef0c6787",
   "metadata": {},
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed76848-7457-4c8f-bd34-ad8b02d053c4",
   "metadata": {},
   "source": [
    "**Gradient Descent** is a fundamental optimization algorithm used in machine learning and other fields to find the minimum of a function, typically a loss or cost function. It plays a crucial role in training machine learning models, especially those involving parameter optimization, such as linear regression, neural networks, and support vector machines. Here's how gradient descent works and its role in machine learning:\n",
    "\n",
    "**Concept of Gradient Descent:**\n",
    "\n",
    "1. **Objective:** Gradient descent is used to minimize a cost or loss function (J(theta)), where theta represents the model's parameters (weights and biases). The goal is to find the values of theta that minimize the cost function.\n",
    "\n",
    "2. **Gradient:** The gradient of the cost function with respect to theta, denoted as (delta)(J(theta)), represents the direction and magnitude of the steepest increase in the cost function. In other words, it tells us how the cost function changes as we make small changes to the parameters.\n",
    "\n",
    "3. **Algorithm:**\n",
    "   - Initialize theta with some initial values.\n",
    "   - Iteratively update theta by taking small steps in the direction opposite to the gradient. This step is known as the \"descent\" step.\n",
    "   - The update rule is given by: theta = theta - alpha(delta)(J(theta)), where alpha is the learning rate, a hyperparameter that controls the step size.\n",
    "\n",
    "4. **Convergence:** Gradient descent continues iterating until one of the following conditions is met:\n",
    "   - The cost function reaches a minimum (convergence).\n",
    "   - A predefined number of iterations is reached.\n",
    "   - The change in theta becomes very small (small gradient norm).\n",
    "\n",
    "**Role in Machine Learning:**\n",
    "\n",
    "Gradient descent is used in various machine learning tasks:\n",
    "\n",
    "1. **Parameter Optimization:** In supervised learning, we often have a model with adjustable parameters (theta), and we want to find the best values of these parameters that minimize the error between predicted and actual outcomes. Gradient descent helps find these optimal parameters.\n",
    "\n",
    "2. **Training Neural Networks:** Neural networks have many parameters, and training them efficiently requires gradient descent. Backpropagation, a variant of gradient descent, is used to update weights in neural networks during training.\n",
    "\n",
    "3. **Regression:** In linear regression, gradient descent is used to find the optimal coefficients that minimize the mean squared error (MSE).\n",
    "\n",
    "4. **Classification:** In logistic regression and support vector machines, gradient descent optimizes the parameters to minimize the logistic loss or hinge loss, respectively.\n",
    "\n",
    "5. **Deep Learning:** In deep learning, deep neural networks have numerous parameters, and gradient descent variants like stochastic gradient descent (SGD), mini-batch gradient descent, and adaptive methods (e.g., Adam) are used to efficiently train these models.\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "To use gradient descent effectively, you need to set hyperparameters, including the learning rate (alpha) and the number of iterations. Choosing appropriate values for these hyperparameters is crucial, as a too-high learning rate can lead to divergence, while a too-low learning rate can result in slow convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924f38f-2d4e-4653-b900-908dcff869ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919c7d20-8e1e-4b43-ba31-be95709f1e23",
   "metadata": {},
   "source": [
    "# Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1b520-9702-4353-85b4-7ade40bb69c7",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression** is an extension of simple linear regression that allows us to model the relationship between a dependent variable Y and two or more independent variables (X_1, X_2, X_3, upto, X_n). In simple linear regression, we only had one independent variable, while in multiple linear regression, we have a linear relationship with multiple predictors. Here's how multiple linear regression differs from simple linear regression:\n",
    "\n",
    "**Simple Linear Regression:**\n",
    "- In simple linear regression, we have one dependent variable Y and one independent variable X.\n",
    "- The model equation is: Y = (theta_0) + (theta_1)X, where theta_0 is the intercept, theta_1 is the coefficient for X.\n",
    "- The goal is to find the best-fit line that minimizes the sum of squared residuals (least squares method).\n",
    "- Simple linear regression models linear relationships between two variables and is represented as a straight line in a 2D space.\n",
    "\n",
    "**Multiple Linear Regression:**\n",
    "- In multiple linear regression, we have one dependent variable Y and two or more independent variables (X_1, X_2, X_3, upto, X_n).\n",
    "- The model equation is: \n",
    "  Y = theta_0 + (theta_1)X_1 + (theta_2)X_2 + ...... + (theta_n)X_n\n",
    "  where theta_0 is the intercept, (theta_1, theta_2, upto, theta_p) are the coefficients for the respective X variables.\n",
    "- The goal is to find the best-fit hyperplane that minimizes the sum of squared residuals.\n",
    "- Multiple linear regression models linear relationships between the dependent variable and multiple predictors and is represented as a hyperplane in a multi-dimensional space.\n",
    "\n",
    "**Key Differences:**\n",
    "1. **Number of Independent Variables:** The primary difference is the number of independent variables. Simple linear regression has only one independent variable, while multiple linear regression has two or more independent variables.\n",
    "\n",
    "2. **Model Representation:** Simple linear regression is represented as a straight line in a 2D space, while multiple linear regression is represented as a hyperplane in a multi-dimensional space.\n",
    "\n",
    "3. **Equation:** The model equation for multiple linear regression includes multiple coefficients (theta_1, theta_2, upto, theta_p) corresponding to each independent variable, whereas simple linear regression has a single coefficient (theta_1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a965c2-d2c7-4757-b0c9-f0d6618c8007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60454e85-2116-4a01-b8ee-79969557a45a",
   "metadata": {},
   "source": [
    "# Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c3f06-99bf-4bf4-888d-4063157f6485",
   "metadata": {},
   "source": [
    "**Multicollinearity** is a common issue that can arise in multiple linear regression when two or more independent variables in the model are highly correlated with each other. It occurs when the independent variables are not independent of each other, making it challenging to determine their individual effects on the dependent variable. Here's a more detailed explanation of multicollinearity and how to detect and address it:\n",
    "\n",
    "**Concept of Multicollinearity:**\n",
    "\n",
    "1. **High Correlation:** Multicollinearity exists when there is a high degree of linear correlation between two or more independent variables in the regression model.\n",
    "\n",
    "2. **Impact on Coefficients:** When multicollinearity is present, it becomes difficult to determine the separate effects of correlated variables because small changes in the data can lead to unstable and unreliable coefficient estimates.\n",
    "\n",
    "**Detection of Multicollinearity:**\n",
    "\n",
    "There are several ways to detect multicollinearity:\n",
    "\n",
    "1. **Correlation Matrix:** Calculate the correlation matrix for the independent variables. High correlation coefficients (e.g., close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2. **VIF (Variance Inflation Factor):** Calculate the VIF for each independent variable. VIF quantifies how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF greater than 1 indicates some level of multicollinearity, with higher values indicating stronger multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity:**\n",
    "\n",
    "Once multicollinearity is detected, here are some strategies to address it:\n",
    "\n",
    "1. **Remove One of the Correlated Variables:** If two or more variables are highly correlated, consider removing one of them from the model. Choose the variable that is less theoretically meaningful or less important in the context of the analysis.\n",
    "\n",
    "2. **Combine Variables:** If it makes sense from a theoretical standpoint, you can create a new variable that is a combination of the highly correlated variables. For example, you can calculate a weighted average or create an interaction term.\n",
    "\n",
    "3. **Regularization Techniques:** Consider using regularization techniques like Ridge or Lasso regression, which can help mitigate the impact of multicollinearity by adding a penalty term to the regression coefficients.\n",
    "\n",
    "4. **Collect More Data:** Increasing the sample size can sometimes help reduce the impact of multicollinearity, but this may not always be feasible.\n",
    "\n",
    "5. **Principal Component Analysis (PCA):** PCA is a dimensionality reduction technique that can transform correlated variables into a set of orthogonal (uncorrelated) variables, effectively addressing multicollinearity.\n",
    "\n",
    "6. **Be Mindful of Model Interpretation:** If multicollinearity cannot be fully resolved, be cautious when interpreting the individual coefficient estimates. Focus on the overall predictive power of the model rather than the precise effect of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e6891-5bd0-42f0-9cf7-067a34584f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a353de-94d8-4265-adae-745ca3872a81",
   "metadata": {},
   "source": [
    "# Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8518413-7750-4d75-9fd0-d31be0bec69e",
   "metadata": {},
   "source": [
    "**Polynomial regression** is a type of regression analysis used to model relationships between a dependent variable and one or more independent variables. It extends the concept of linear regression by allowing the relationship between the variables to be modeled as an nth-degree polynomial. Here's a detailed explanation of polynomial regression and how it differs from linear regression:\n",
    "\n",
    "**Polynomial Regression:**\n",
    "\n",
    "1. **Polynomial Equation:** In polynomial regression, the relationship between the dependent variable (Y) and the independent variable (X) is modeled as a polynomial equation of degree n:\n",
    "   Y = theta_0 + (theta_1)X + (theta_2)X^2 + (theta_3)X^3 + ..... + (theta_n)X^n\n",
    "   - n represents the degree of the polynomial, which determines the number of terms in the equation.\n",
    "   - theta_0, theta_1, theta_2, upto , theta_n are the coefficients to be estimated.\n",
    "\n",
    "2. **Non-Linear Relationships:** Polynomial regression is suitable for modeling non-linear relationships between variables. It allows the curve to fit the data more flexibly than a straight line, making it useful when the relationship is curvilinear.\n",
    "\n",
    "3. **Degree of the Polynomial:** The choice of the degree (n) of the polynomial is a critical decision in polynomial regression. Higher-degree polynomials can capture more complex patterns but can also lead to overfitting.\n",
    "\n",
    "4. **Interpretation:** The interpretation of coefficients becomes more complex in polynomial regression as each coefficient corresponds to a term in the polynomial equation. The effect of X on Y is not constant but varies with the degree of the polynomial.\n",
    "\n",
    "**Differences from Linear Regression:**\n",
    "\n",
    "1. **Linearity:** In linear regression, the relationship between variables is modeled as a linear equation (a straight line). In contrast, polynomial regression models non-linear relationships using polynomial equations.\n",
    "\n",
    "2. **Complexity:** Polynomial regression introduces more complexity, especially as the degree of the polynomial (n) increases. This complexity allows the model to capture intricate patterns but can also lead to overfitting if not carefully chosen.\n",
    "\n",
    "3. **Interpretation:** Interpretation of coefficients is simpler in linear regression, where each coefficient represents the change in the dependent variable for a one-unit change in the independent variable. In polynomial regression, interpretation becomes more challenging as it depends on the degree and the specific coefficients in the polynomial equation.\n",
    "\n",
    "4. **Underfitting and Overfitting:** In linear regression, there is a risk of underfitting if the relationship is non-linear. Polynomial regression can address this issue but is susceptible to overfitting when using high-degree polynomials.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Polynomial regression is useful when the relationship between variables exhibits a curve or when linear regression does not adequately capture the underlying patterns.\n",
    "- It is commonly used in fields like economics, physics, biology, and engineering to model complex, non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794e906-a1e9-4820-884a-e3236120d584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eeada0b-9634-4760-a85b-7f205b61256d",
   "metadata": {},
   "source": [
    "# Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7396e-8021-4013-8fe4-1d5f8b4b9f38",
   "metadata": {},
   "source": [
    "**Advantages of Polynomial Regression Compared to Linear Regression:**\n",
    "\n",
    "1. **Flexibility:** Polynomial regression is more flexible than linear regression because it can model non-linear relationships between variables. It can capture complex patterns and curves in the data.\n",
    "\n",
    "2. **Better Fit to Data:** In cases where the true relationship between variables is non-linear, polynomial regression can provide a better fit to the data compared to linear regression. It can reduce the residual errors and improve the model's predictive power.\n",
    "\n",
    "3. **Enhanced Accuracy:** Polynomial regression can lead to more accurate predictions when the underlying data-generating process is non-linear. It allows the model to approximate the true relationship more closely.\n",
    "\n",
    "**Disadvantages of Polynomial Regression Compared to Linear Regression:**\n",
    "\n",
    "1. **Overfitting:** One of the main disadvantages of polynomial regression is its susceptibility to overfitting, especially when using high-degree polynomials. High-degree polynomials can fit noise in the data and lead to poor generalization to new, unseen data.\n",
    "\n",
    "2. **Interpretation Complexity:** Polynomial regression models are less interpretable compared to linear regression. Coefficients represent the impact of terms in the polynomial equation, making interpretation less straightforward.\n",
    "\n",
    "3. **Data Requirements:** Polynomial regression may require larger datasets to accurately estimate the coefficients of higher-degree terms. Smaller datasets can result in unstable and unreliable coefficient estimates.\n",
    "\n",
    "**Situations to Prefer Polynomial Regression:**\n",
    "\n",
    "1. **Non-Linear Relationships:** Use polynomial regression when you believe that the relationship between the dependent and independent variables is non-linear. It can capture curves, bends, and complex patterns in the data.\n",
    "\n",
    "2. **Exploratory Analysis:** Polynomial regression is valuable during exploratory data analysis when you are unsure about the linearity of the relationship. It can help you visualize and model potential non-linear trends in the data.\n",
    "\n",
    "3. **Predictive Accuracy:** Choose polynomial regression when predictive accuracy is crucial, and a linear model does not adequately fit the data. In such cases, polynomial regression can improve prediction accuracy.\n",
    "\n",
    "4. **Experimental Data:** In some experimental or scientific contexts, polynomial regression may be appropriate when you expect the underlying physical or biological processes to follow non-linear patterns.\n",
    "\n",
    "5. **Feature Engineering:** Polynomial regression can be used as a feature engineering technique to create higher-order polynomial features for linear regression models. This can help capture non-linear effects while retaining the interpretability of linear models.\n",
    "\n",
    "6. **Regularization:** When using regularization techniques like Ridge or Lasso regression, polynomial regression can help prevent underfitting by introducing non-linearity into the model while controlling the complexity of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34670405-f3fa-4f5b-9d04-6448c2184d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd134a1-c018-4f58-90f7-95f09bbe5524",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
