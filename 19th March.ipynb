{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to scale and transform the values of numerical features within a specific range. The goal is to bring all the features to a similar scale, preventing some features from dominating others due to differences in their original magnitudes. This is particularly important for machine learning algorithms that are sensitive to the scale of input features, such as gradient descent-based methods.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    " X_((scaled)) = (X - X_((min))) / (X_((max)) - X_((min))) \n",
    "\n",
    "Where:\n",
    "-  X_((scaled))  is the scaled value of the feature.\n",
    "-  X  is the original value of the feature.\n",
    "-  X_((min))  is the minimum value of the feature in the dataset.\n",
    "-  X_((max))  is the maximum value of the feature in the dataset.\n",
    "\n",
    "The resulting scaled values will fall within the range [0, 1].\n",
    "\n",
    "In below example, the `MinMaxScaler` from scikit-learn is used to scale the dataset. The `fit_transform` method computes the minimum and maximum values of each feature in the dataset and scales the values accordingly. The resulting `scaled_data` will have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "Scaled data:\n",
      " [[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\\n\", data)\n",
    "print(\"Scaled data:\\n\", scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or unit normalization, is a feature scaling method that scales the values of a feature to have a unit norm. The unit norm is the Euclidean norm (L2 norm) of a vector, which is the square root of the sum of the squared elements. The purpose of unit normalization is to ensure that the magnitude of each feature vector is equal to 1.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    " X_((unit)) = (X) / (||X||_2) \n",
    "\n",
    "Where:\n",
    "-  X_((unit))  is the unit-scaled vector.\n",
    "-  X  is the original vector of feature values.\n",
    "-  ||X||_2  is the Euclidean norm of the vector  X .\n",
    "\n",
    "Unlike Min-Max scaling, which scales the values of each feature to a specific range (e.g., [0, 1]), Unit Vector scaling focuses on the direction of the vector, not the magnitude.\n",
    "\n",
    "In below example, the `Normalizer` from scikit-learn is used with the L2 norm. The `transform` method normalizes each row (feature vector) in the dataset to have a unit norm. The resulting `unit_scaled_data` will have feature vectors with a magnitude of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "Unit Vector scaled data:\n",
      " [[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]\n",
      " [0.50257071 0.57436653 0.64616234]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "normalizer = Normalizer(norm='l2')\n",
    "unit_scaled_data = normalizer.transform(data)\n",
    "print(\"Original data:\\n\", data)\n",
    "print(\"Unit Vector scaled data:\\n\", unit_scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is commonly used in machine learning and data analysis. The main goal of PCA is to transform a dataset into a new coordinate system, where the axes are the principal components, and the data is represented in terms of these components. The principal components are linear combinations of the original features, and they are arranged in descending order of variance. By retaining only the top k principal components, where k is a user-defined parameter, PCA reduces the dimensionality of the data while preserving most of its variability.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. Standardize the data: If the features are on different scales, it's common practice to standardize the data (subtract mean and divide by standard deviation).\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "4. Sort and select top k components: Sort the eigenvectors by their corresponding eigenvalues in descending order. Choose the top k eigenvectors to form the new feature subspace.\n",
    "\n",
    "5. Transform the data: Use the selected eigenvectors to transform the original data into the new feature space.\n",
    "\n",
    "In below example, PCA is applied to the Iris dataset. The data is standardized, and then PCA is used to transform it into a new space with only 2 principal components. The resulting `X_pca` contains the data in this reduced-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (150, 4)\n",
      "Transformed data shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "print(\"Original data shape:\", X_standardized.shape)\n",
    "print(\"Transformed data shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction, and in fact, PCA is a specific technique for feature extraction. Feature extraction is a broader concept that encompasses various methods to transform and represent the original features of a dataset in a different, often more compact, and informative way. PCA is one such technique for feature extraction, and it specifically focuses on capturing the most significant patterns in the data by identifying principal components.\n",
    "\n",
    "Below is how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Linear Combination of Features:**\n",
    "   - PCA identifies linear combinations of the original features, known as principal components.\n",
    "   - These principal components are orthogonal to each other and are ranked by the amount of variance they capture in the data.\n",
    "\n",
    "2. **Dimensionality Reduction:**\n",
    "   - By selecting a subset of the top principal components, PCA reduces the dimensionality of the dataset.\n",
    "   - This reduction in dimensionality is a form of feature extraction, as it transforms the original features into a smaller set of more meaningful components.\n",
    "\n",
    "3. **Preservation of Variance:**\n",
    "   - PCA retains as much variance in the data as possible in the reduced-dimensional space.\n",
    "   - The first few principal components capture the most significant sources of variability in the data, providing a compact representation.\n",
    "\n",
    "In this example, PCA is applied to the Iris dataset, and the number of components is set to 2. The resulting `X_features` contains the data in a reduced-dimensional space, where the original features have been effectively transformed into two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (150, 4)\n",
      "Transformed data shape (after feature extraction): (150, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "X_features = pca.fit_transform(X_standardized)\n",
    "\n",
    "print(\"Original data shape:\", X_standardized.shape)\n",
    "print(\"Transformed data shape (after feature extraction):\", X_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to standardize the range of values for each feature, ensuring that they fall within a specific interval (typically [0, 1]). By applying Min-Max scaling, we ensure that the features with different scales (e.g., price in dollars and rating on a scale of 1 to 5) are transformed to a common scale, making them comparable. This is important for recommendation systems, as it prevents features with larger magnitudes from dominating the recommendation process.\n",
    "\n",
    "1. **Understand the Data:**\n",
    "   - Examine the range of values for each feature in the dataset (price, rating, delivery time).\n",
    "   - Identify if there are significant differences in the magnitudes of these features.\n",
    "\n",
    "2. **Import Necessary Libraries:**\n",
    "   - In Python, we might use libraries such as NumPy or scikit-learn for Min-Max scaling. Here's an example using scikit-learn:\n",
    "\n",
    "     ```python\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "     ```\n",
    "\n",
    "3. **Standardize the Data:**\n",
    "   - Create an instance of the `MinMaxScaler` class.\n",
    "   - Apply the `fit_transform` method to scale the features within the desired range.\n",
    "\n",
    "     ```python\n",
    "     # Assuming 'data' is your dataset with columns for price, rating, and delivery time\n",
    "     scaler = MinMaxScaler()\n",
    "     scaled_data = scaler.fit_transform(data)\n",
    "     ```\n",
    "\n",
    "4. **Check the Scaled Data:**\n",
    "   - Inspect the resulting scaled data to ensure that the values are now within the [0, 1] range.\n",
    "\n",
    "     ```python\n",
    "     print(\"Original data:\\n\", data)\n",
    "     print(\"Scaled data:\\n\", scaled_data)\n",
    "     ```\n",
    "\n",
    "5. **Updated Dataset:**\n",
    "   - Replace the original values with the scaled values in your dataset.\n",
    "\n",
    "     ```python\n",
    "     data['price'] = scaled_data[:, 0]  # Assuming price is the first column\n",
    "     data['rating'] = scaled_data[:, 1]  # Assuming rating is the second column\n",
    "     data['delivery_time'] = scaled_data[:, 2]  # Assuming delivery time is the third column\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset containing numerous features like company financial data and market trends, Principal Component Analysis (PCA) can be used to reduce the dimensionality of the dataset. Dimensionality reduction is beneficial in this context for several reasons, including mitigating the curse of dimensionality, simplifying the model, and potentially improving the model's generalization performance.\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Handle missing values, outliers, and any other data preprocessing steps as needed.\n",
    "   - Standardize the features to ensure that they have a mean of 0 and a standard deviation of 1. This step is essential for PCA, as it is sensitive to the scale of the features.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Import the necessary libraries, such as scikit-learn in Python:\n",
    "\n",
    "     ```python\n",
    "     from sklearn.decomposition import PCA\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "     ```\n",
    "\n",
    "   - Standardize the data:\n",
    "\n",
    "     ```python\n",
    "     # Assuming 'data' with multiple features\n",
    "     scaler = StandardScaler()\n",
    "     standardized_data = scaler.fit_transform(data)\n",
    "     ```\n",
    "\n",
    "   - Apply PCA to the standardized data:\n",
    "\n",
    "     ```python\n",
    "     # Choose the number of components\n",
    "     pca = PCA(n_components=5)\n",
    "     reduced_data = pca.fit_transform(standardized_data)\n",
    "     ```\n",
    "\n",
    "   - The `n_components` parameter specifies the number of principal components to retain. we can choose this value based on the desired level of dimensionality reduction.\n",
    "\n",
    "3. **Explained Variance:**\n",
    "   - Analyze the explained variance to understand how much of the total variance in the original dataset is retained by each principal component:\n",
    "\n",
    "     ```python\n",
    "     explained_variance_ratio = pca.explained_variance_ratio_\n",
    "     print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "     ```\n",
    "\n",
    "   - This information helps we decide how many principal components to retain. we may choose a number that retains a significant portion of the variance, such as 95% or 99%.\n",
    "\n",
    "4. **Select Number of Components:**\n",
    "   - Based on the analysis of explained variance, choose the appropriate number of principal components that retain a sufficient amount of information.\n",
    "\n",
    "5. **Update Dataset:**\n",
    "   - Use the selected number of principal components to update your dataset:\n",
    "\n",
    "     ```python\n",
    "     # 'new_data' is your updated dataset with reduced dimensionality\n",
    "     new_data = reduced_data[:, :chosen_number_of_components]\n",
    "     ```\n",
    "\n",
    "   - This reduced dataset can then be used as input for your stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_max_scale(data):\n",
    "    x_norm = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    x_scaled = 2 * x_norm - 1\n",
    "    return x_scaled\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "scaled_data = min_max_scale(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision on how many principal components to retain during Feature Extraction using PCA involves a trade-off between reducing dimensionality and retaining a sufficient amount of variance in the data. The goal is to capture as much information as possible with a smaller set of features. \n",
    "\n",
    "The rationale for choosing the number of principal components depends on the application and the desired balance between dimensionality reduction and information preservation. Retaining more components captures more information but might increase computational complexity, while retaining fewer components simplifies the model but may lose some information. Below are the steps we can follow to determine the number of principal components to retain:\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   - Before applying PCA, it's important to standardize the data to ensure that all features are on the same scale.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Use PCA to transform the standardized data and compute the principal components.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.decomposition import PCA\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   # Assuming 'data' is dataset with columns for height, weight, age, gender, blood pressure\n",
    "   scaler = StandardScaler()\n",
    "   standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "   pca = PCA()\n",
    "   principal_components = pca.fit_transform(standardized_data)\n",
    "   ```\n",
    "\n",
    "3. **Explained Variance:**\n",
    "   - Analyze the explained variance ratio to understand how much variance is captured by each principal component.\n",
    "\n",
    "   ```python\n",
    "   explained_variance_ratio = pca.explained_variance_ratio_\n",
    "   ```\n",
    "\n",
    "4. **Cumulative Explained Variance:**\n",
    "   - Calculate the cumulative explained variance to see how much variance is explained as we include more principal components.\n",
    "\n",
    "   ```python\n",
    "   cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "   ```\n",
    "\n",
    "5. **Choose the Number of Components:**\n",
    "   - Decide on the number of principal components to retain based on the cumulative explained variance and your desired threshold.\n",
    "   - Common choices include retaining enough components to explain 90%, 95%, or 99% of the total variance.\n",
    "\n",
    "   ```python\n",
    "   # Retaining components explaining 95% of the variance\n",
    "   n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "   ```\n",
    "\n",
    "6. **Update Dataset:**\n",
    "   - Use the selected number of principal components to update your dataset.\n",
    "\n",
    "   ```python\n",
    "   # 'new_data' is updated dataset with reduced dimensionality\n",
    "   new_data = principal_components[:, :n_components]\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
